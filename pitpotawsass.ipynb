{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "254f7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9df8e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client(service_name='s3',\n",
    "aws_access_key_id='AKIAUSEODYLG6RNKOVEM',\n",
    "aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cc0170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lis = []\n",
    "for j in range(1, 4):\n",
    "    obj = s3.get_object(Bucket = 'pipozbuck',Key = 'txt'+str(j)+'.txt')\n",
    "    obj['Body']\n",
    "    data = ''\n",
    "    with io.FileIO('sample.txt', 'w') as file:\n",
    "        for i in obj['Body']:\n",
    "            file.write(i)\n",
    "    f = open(\"sample.txt\", \"r\")\n",
    "    data_lis.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e65a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy performance metrics can be decisive when dealing with imbalanced data. In this blog, we will learn about the Confusion matrix and its associated terms, which looks confusing but are trivial. The confusion matrix, precision, recall, and F1 score gives better intuition of prediction results as compared to accuracy. To understand the concepts, we will limit this article to binary classification only.\\n\\nWhat is a confusion matrix?\\nIt is a matrix of size 2×2 for binary classification with actual values on one axis and predicted on another.\\n\\nConfusion Matrix\\nLet’s understand the confusing terms in the confusion matrix: true positive, true negative, false negative, and false positive with an example.\\n\\nEXAMPLE\\n\\nA machine learning model is trained to predict tumor in patients. The test dataset consists of 100 people.\\n\\n\\nConfusion Matrix for tumor detection\\nTrue Positive (TP) — model correctly predicts the positive class (prediction and actual both are positive). In the above example, 10 people who have tumors are predicted positively by the model.\\nTrue Negative (TN) — model correctly predicts the negative class (prediction and actual both are negative). In the above example, 60 people who don’t have tumors are predicted negatively by the model.\\nFalse Positive (FP) — model gives the wrong prediction of the negative class (predicted-positive, actual-negative). In the above example, 22 people are predicted as positive of having a tumor, although they don’t have a tumor. FP is also called a TYPE I error.\\nFalse Negative (FN) — model wrongly predicts the positive class (predicted-negative, actual-positive). In the above example, 8 people who have tumors are predicted as negative. FN is also called a TYPE II error.\\n\\nWith the help of these four values, we can calculate True Positive Rate (TPR), False Negative Rate (FPR), True Negative Rate (TNR), and False Negative Rate (FNR).\\n\\n\\n\\n',\n",
       " 'Even if data is imbalanced, we can figure out that our model is working well or not. For that, the values of TPR and TNR should be high, and FPR and FNR should be as low as possible.\\n\\nWith the help of TP, TN, FN, and FP, other performance metrics can be calculated.\\n\\nPrecision, Recall\\nBoth precision and recall are crucial for information retrieval, where positive class mattered the most as compared to negative. Why?\\n\\nWhile searching something on the web, the model does not care about something irrelevant and not retrieved (this is the true negative case). Therefore only TP, FP, FN are used in Precision and Recall.\\n\\nPrecision\\n\\nOut of all the positive predicted, what percentage is truly positive.\\n\\n\\nThe precision value lies between 0 and 1.\\n\\nRecall\\n\\nOut of the total positive, what percentage are predicted positive. It is the same as TPR (true positive rate).\\n\\n\\nHow are precision and recall useful? Let’s see through examples.\\n\\nEXAMPLE 1- Credit card fraud detection\\n\\n\\nConfusion Matrix for Credit Card Fraud Detection\\nWe do not want to miss any fraud transactions. Therefore, we want False-Negative to be as low as possible. In these situations, we can compromise with the low precision, but recall should be high. Similarly, in the medical application, we don’t want to miss any patient. Therefore we focus on having a high recall.\\n\\nSo far, we have discussed when the recall is important than precision. But, when is the precision more important than recall?\\n\\nEXAMPLE 2 — Spam detection\\n\\n\\nConfusion Matrix for Spam detection\\nIn the detection of spam mail, it is okay if any spam mail remains undetected (false negative), but what if we miss any critical mail because it is classified as spam (false positive). In this situation, False Positive should be as low as possible. Here, precision is more vital as compared to recall.\\n\\nWhen comparing different models, it will be difficult to decide which is better (high precision and low recall or vice-versa). Therefore, there should be a metric that combines both of these. One such metric is the F1 score.\\n',\n",
       " 'F1 Score\\nIt is the harmonic mean of precision and recall. It takes both false positive and false negatives into account. Therefore, it performs well on an imbalanced dataset.\\n\\n\\nF1 score gives the same weightage to recall and precision.\\n\\nThere is a weighted F1 score in which we can give different weightage to recall and precision. As discussed in the previous section, different problems give different weightage to recall and precision.\\n\\n\\nBeta represents how many times recall is more important than precision. If the recall is twice as important as precision, the value of Beta is 2.\\n\\nConclusion\\nConfusion matrix, precision, recall, and F1 score provides better insights into the prediction as compared to accuracy performance metrics. Applications of precision, recall, and F1 score is in information retrieval, word segmentation, named entity recognition, and many more.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "660b5c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy performance metrics can be decisive when dealing with imbalanced data. In this blog, we will learn about the Confusion matrix and its associated terms, which looks confusing but are trivial. The confusion matrix, precision, recall, and F1 score gives better intuition of prediction results as compared to accuracy. To understand the concepts, we will limit this article to binary classification only.\\n\\nWhat is a confusion matrix?\\nIt is a matrix of size 2×2 for binary classification with actual values on one axis and predicted on another.\\n\\nConfusion Matrix\\nLet’s understand the confusing terms in the confusion matrix: true positive, true negative, false negative, and false positive with an example.\\n\\nEXAMPLE\\n\\nA machine learning model is trained to predict tumor in patients. The test dataset consists of 100 people.\\n\\n\\nConfusion Matrix for tumor detection\\nTrue Positive (TP) — model correctly predicts the positive class (prediction and actual both are positive). In the above example, 10 people who have tumors are predicted positively by the model.\\nTrue Negative (TN) — model correctly predicts the negative class (prediction and actual both are negative). In the above example, 60 people who don’t have tumors are predicted negatively by the model.\\nFalse Positive (FP) — model gives the wrong prediction of the negative class (predicted-positive, actual-negative). In the above example, 22 people are predicted as positive of having a tumor, although they don’t have a tumor. FP is also called a TYPE I error.\\nFalse Negative (FN) — model wrongly predicts the positive class (predicted-negative, actual-positive). In the above example, 8 people who have tumors are predicted as negative. FN is also called a TYPE II error.\\n\\nWith the help of these four values, we can calculate True Positive Rate (TPR), False Negative Rate (FPR), True Negative Rate (TNR), and False Negative Rate (FNR).\\n\\n\\n\\nEven if data is imbalanced, we can figure out that our model is working well or not. For that, the values of TPR and TNR should be high, and FPR and FNR should be as low as possible.\\n\\nWith the help of TP, TN, FN, and FP, other performance metrics can be calculated.\\n\\nPrecision, Recall\\nBoth precision and recall are crucial for information retrieval, where positive class mattered the most as compared to negative. Why?\\n\\nWhile searching something on the web, the model does not care about something irrelevant and not retrieved (this is the true negative case). Therefore only TP, FP, FN are used in Precision and Recall.\\n\\nPrecision\\n\\nOut of all the positive predicted, what percentage is truly positive.\\n\\n\\nThe precision value lies between 0 and 1.\\n\\nRecall\\n\\nOut of the total positive, what percentage are predicted positive. It is the same as TPR (true positive rate).\\n\\n\\nHow are precision and recall useful? Let’s see through examples.\\n\\nEXAMPLE 1- Credit card fraud detection\\n\\n\\nConfusion Matrix for Credit Card Fraud Detection\\nWe do not want to miss any fraud transactions. Therefore, we want False-Negative to be as low as possible. In these situations, we can compromise with the low precision, but recall should be high. Similarly, in the medical application, we don’t want to miss any patient. Therefore we focus on having a high recall.\\n\\nSo far, we have discussed when the recall is important than precision. But, when is the precision more important than recall?\\n\\nEXAMPLE 2 — Spam detection\\n\\n\\nConfusion Matrix for Spam detection\\nIn the detection of spam mail, it is okay if any spam mail remains undetected (false negative), but what if we miss any critical mail because it is classified as spam (false positive). In this situation, False Positive should be as low as possible. Here, precision is more vital as compared to recall.\\n\\nWhen comparing different models, it will be difficult to decide which is better (high precision and low recall or vice-versa). Therefore, there should be a metric that combines both of these. One such metric is the F1 score.\\nF1 Score\\nIt is the harmonic mean of precision and recall. It takes both false positive and false negatives into account. Therefore, it performs well on an imbalanced dataset.\\n\\n\\nF1 score gives the same weightage to recall and precision.\\n\\nThere is a weighted F1 score in which we can give different weightage to recall and precision. As discussed in the previous section, different problems give different weightage to recall and precision.\\n\\n\\nBeta represents how many times recall is more important than precision. If the recall is twice as important as precision, the value of Beta is 2.\\n\\nConclusion\\nConfusion matrix, precision, recall, and F1 score provides better insights into the prediction as compared to accuracy performance metrics. Applications of precision, recall, and F1 score is in information retrieval, word segmentation, named entity recognition, and many more.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ''\n",
    "for s in data_lis:\n",
    "    data += s\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdefe623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.3.1)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (4.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathy>=0.3.5->spacy) (0.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.14->spacy) (2.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: immutables>=0.9 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.14->spacy) (0.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f4b2404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     |████████████████████████████████| 12.8 MB 4.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.18.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.26.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.14->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.1.1)\n",
      "Requirement already satisfied: immutables>=0.9 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.14->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.15)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac8ac2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy performance metrics can be decisive when dealing with imbalanced data.',\n",
       " 'In this blog, we will learn about the Confusion matrix and its associated terms, which looks confusing but are trivial.',\n",
       " 'The confusion matrix, precision, recall, and F1 score gives better intuition of prediction results as compared to accuracy.',\n",
       " 'To understand the concepts, we will limit this article to binary classification only.',\n",
       " '\\n\\nWhat is a confusion matrix?',\n",
       " '\\nIt is a matrix of size 2×2 for binary classification with actual values on one axis and predicted on another.',\n",
       " '\\n\\nConfusion Matrix\\nLet’s understand the confusing terms in the confusion matrix: true positive, true negative, false negative, and false positive with an example.',\n",
       " '\\n\\nEXAMPLE\\n\\nA machine learning model is trained to predict tumor in patients.',\n",
       " 'The test dataset consists of 100 people.',\n",
       " '\\n\\n\\nConfusion Matrix for tumor detection\\nTrue Positive (TP) — model correctly predicts the positive class (prediction and actual both are positive).',\n",
       " 'In the above example, 10 people who have tumors are predicted positively by the model.',\n",
       " '\\nTrue Negative (TN) — model correctly predicts the negative class (prediction and actual both are negative).',\n",
       " 'In the above example, 60 people who don’t have tumors are predicted negatively by the model.',\n",
       " '\\nFalse Positive (FP) — model gives the wrong prediction of the negative class (predicted-positive, actual-negative).',\n",
       " 'In the above example, 22 people are predicted as positive of having a tumor, although they don’t have a tumor.',\n",
       " 'FP is also called a TYPE I error.',\n",
       " '\\nFalse Negative (FN) — model wrongly predicts the positive class (predicted-negative, actual-positive).',\n",
       " 'In the above example, 8 people who have tumors are predicted as negative.',\n",
       " 'FN is also called a TYPE II error.',\n",
       " '\\n\\nWith the help of these four values, we can calculate True Positive Rate (TPR), False Negative Rate (FPR), True Negative Rate (TNR), and False Negative Rate (FNR).',\n",
       " '\\n\\n\\n\\nEven if data is imbalanced, we can figure out that our model is working well or not.',\n",
       " 'For that, the values of TPR and TNR should be high, and FPR and FNR should be as low as possible.',\n",
       " '\\n\\nWith the help of TP, TN, FN, and FP, other performance metrics can be calculated.',\n",
       " '\\n\\nPrecision, Recall\\nBoth precision and recall are crucial for information retrieval, where positive class mattered the most as compared to negative.',\n",
       " 'Why?',\n",
       " '\\n\\nWhile searching something on the web, the model does not care about something irrelevant and not retrieved (this is the true negative case).',\n",
       " 'Therefore only TP, FP, FN are used in Precision and Recall.',\n",
       " '\\n\\nPrecision\\n\\nOut of all the positive predicted, what percentage is truly positive.',\n",
       " '\\n\\n\\nThe precision value lies between 0 and 1.',\n",
       " '\\n\\nRecall\\n\\nOut of the total positive, what percentage are predicted positive.',\n",
       " 'It is the same as TPR (true positive rate).',\n",
       " '\\n\\n\\nHow are precision and recall useful?',\n",
       " 'Let’s see through examples.',\n",
       " '\\n\\nEXAMPLE 1- Credit card fraud detection\\n\\n\\nConfusion Matrix for Credit Card Fraud Detection\\nWe do not want to miss any fraud transactions.',\n",
       " 'Therefore, we want False-Negative to be as low as possible.',\n",
       " 'In these situations, we can compromise with the low precision, but recall should be high.',\n",
       " 'Similarly, in the medical application, we don’t want to miss any patient.',\n",
       " 'Therefore we focus on having a high recall.',\n",
       " '\\n\\nSo far, we have discussed when the recall is important than precision.',\n",
       " 'But, when is the precision more important than recall?',\n",
       " '\\n\\nEXAMPLE 2 — Spam detection\\n\\n\\nConfusion Matrix for Spam detection\\nIn the detection of spam mail, it is okay if any spam mail remains undetected (false negative), but what if we miss any critical mail because it is classified as spam (false positive).',\n",
       " 'In this situation, False Positive should be as low as possible.',\n",
       " 'Here, precision is more vital as compared to recall.',\n",
       " '\\n\\nWhen comparing different models, it will be difficult to decide which is better (high precision and low recall or vice-versa).',\n",
       " 'Therefore, there should be a metric that combines both of these.',\n",
       " 'One such metric is the F1 score.',\n",
       " '\\nF1 Score\\nIt is the harmonic mean of precision and recall.',\n",
       " 'It takes both false positive and false negatives into account.',\n",
       " 'Therefore, it performs well on an imbalanced dataset.',\n",
       " '\\n\\n\\nF1 score gives the same weightage to recall and precision.',\n",
       " '\\n\\nThere is a weighted F1 score in which we can give different weightage to recall and precision.',\n",
       " 'As discussed in the previous section, different problems give different weightage to recall and precision.',\n",
       " '\\n\\n\\nBeta represents how many times recall is more important than precision.',\n",
       " 'If the recall is twice as important as precision, the value of Beta is 2.',\n",
       " '\\n\\nConclusion\\nConfusion matrix, precision, recall, and F1 score provides better insights into the prediction as compared to accuracy performance metrics.',\n",
       " 'Applications of precision, recall, and F1 score is in information retrieval, word segmentation, named entity recognition, and many more.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "  \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "  \n",
    "doc = nlp(data)\n",
    "\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(str(sent))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "662c1b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c0a11cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Accuracy',\n",
       " 'performance',\n",
       " 'metrics',\n",
       " 'decisive',\n",
       " 'dealing',\n",
       " 'imbalanced',\n",
       " 'data',\n",
       " '.',\n",
       " 'In',\n",
       " 'blog',\n",
       " ',',\n",
       " 'learn',\n",
       " 'Confusion',\n",
       " 'matrix',\n",
       " 'associated',\n",
       " 'terms',\n",
       " ',',\n",
       " 'looks',\n",
       " 'confusing',\n",
       " 'trivial',\n",
       " '.',\n",
       " 'The',\n",
       " 'confusion',\n",
       " 'matrix',\n",
       " ',',\n",
       " 'precision',\n",
       " ',',\n",
       " 'recall',\n",
       " ',',\n",
       " 'F1',\n",
       " 'score',\n",
       " 'gives',\n",
       " 'better',\n",
       " 'intuition',\n",
       " 'prediction',\n",
       " 'results',\n",
       " 'compared',\n",
       " 'accuracy',\n",
       " '.',\n",
       " 'To',\n",
       " 'understand',\n",
       " 'concepts',\n",
       " ',',\n",
       " 'limit',\n",
       " 'article',\n",
       " 'binary',\n",
       " 'classification',\n",
       " '.',\n",
       " 'What',\n",
       " 'confusion',\n",
       " 'matrix',\n",
       " '?',\n",
       " 'It',\n",
       " 'matrix',\n",
       " 'size',\n",
       " '2×2',\n",
       " 'binary',\n",
       " 'classification',\n",
       " 'actual',\n",
       " 'values',\n",
       " 'axis',\n",
       " 'predicted',\n",
       " 'another',\n",
       " '.',\n",
       " 'Confusion',\n",
       " 'Matrix',\n",
       " 'Let',\n",
       " '’',\n",
       " 'understand',\n",
       " 'confusing',\n",
       " 'terms',\n",
       " 'confusion',\n",
       " 'matrix',\n",
       " ':',\n",
       " 'true',\n",
       " 'positive',\n",
       " ',',\n",
       " 'true',\n",
       " 'negative',\n",
       " ',',\n",
       " 'false',\n",
       " 'negative',\n",
       " ',',\n",
       " 'false',\n",
       " 'positive',\n",
       " 'example',\n",
       " '.',\n",
       " 'EXAMPLE',\n",
       " 'A',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'model',\n",
       " 'trained',\n",
       " 'predict',\n",
       " 'tumor',\n",
       " 'patients',\n",
       " '.',\n",
       " 'The',\n",
       " 'test',\n",
       " 'dataset',\n",
       " 'consists',\n",
       " '100',\n",
       " 'people',\n",
       " '.',\n",
       " 'Confusion',\n",
       " 'Matrix',\n",
       " 'tumor',\n",
       " 'detection',\n",
       " 'True',\n",
       " 'Positive',\n",
       " '(',\n",
       " 'TP',\n",
       " ')',\n",
       " '—',\n",
       " 'model',\n",
       " 'correctly',\n",
       " 'predicts',\n",
       " 'positive',\n",
       " 'class',\n",
       " '(',\n",
       " 'prediction',\n",
       " 'actual',\n",
       " 'positive',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'example',\n",
       " ',',\n",
       " '10',\n",
       " 'people',\n",
       " 'tumors',\n",
       " 'predicted',\n",
       " 'positively',\n",
       " 'model',\n",
       " '.',\n",
       " 'True',\n",
       " 'Negative',\n",
       " '(',\n",
       " 'TN',\n",
       " ')',\n",
       " '—',\n",
       " 'model',\n",
       " 'correctly',\n",
       " 'predicts',\n",
       " 'negative',\n",
       " 'class',\n",
       " '(',\n",
       " 'prediction',\n",
       " 'actual',\n",
       " 'negative',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'example',\n",
       " ',',\n",
       " '60',\n",
       " 'people',\n",
       " '’',\n",
       " 'tumors',\n",
       " 'predicted',\n",
       " 'negatively',\n",
       " 'model',\n",
       " '.',\n",
       " 'False',\n",
       " 'Positive',\n",
       " '(',\n",
       " 'FP',\n",
       " ')',\n",
       " '—',\n",
       " 'model',\n",
       " 'gives',\n",
       " 'wrong',\n",
       " 'prediction',\n",
       " 'negative',\n",
       " 'class',\n",
       " '(',\n",
       " 'predicted-positive',\n",
       " ',',\n",
       " 'actual-negative',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'example',\n",
       " ',',\n",
       " '22',\n",
       " 'people',\n",
       " 'predicted',\n",
       " 'positive',\n",
       " 'tumor',\n",
       " ',',\n",
       " 'although',\n",
       " '’',\n",
       " 'tumor',\n",
       " '.',\n",
       " 'FP',\n",
       " 'called',\n",
       " 'TYPE',\n",
       " 'I',\n",
       " 'error',\n",
       " '.',\n",
       " 'False',\n",
       " 'Negative',\n",
       " '(',\n",
       " 'FN',\n",
       " ')',\n",
       " '—',\n",
       " 'model',\n",
       " 'wrongly',\n",
       " 'predicts',\n",
       " 'positive',\n",
       " 'class',\n",
       " '(',\n",
       " 'predicted-negative',\n",
       " ',',\n",
       " 'actual-positive',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'example',\n",
       " ',',\n",
       " '8',\n",
       " 'people',\n",
       " 'tumors',\n",
       " 'predicted',\n",
       " 'negative',\n",
       " '.',\n",
       " 'FN',\n",
       " 'called',\n",
       " 'TYPE',\n",
       " 'II',\n",
       " 'error',\n",
       " '.',\n",
       " 'With',\n",
       " 'help',\n",
       " 'four',\n",
       " 'values',\n",
       " ',',\n",
       " 'calculate',\n",
       " 'True',\n",
       " 'Positive',\n",
       " 'Rate',\n",
       " '(',\n",
       " 'TPR',\n",
       " ')',\n",
       " ',',\n",
       " 'False',\n",
       " 'Negative',\n",
       " 'Rate',\n",
       " '(',\n",
       " 'FPR',\n",
       " ')',\n",
       " ',',\n",
       " 'True',\n",
       " 'Negative',\n",
       " 'Rate',\n",
       " '(',\n",
       " 'TNR',\n",
       " ')',\n",
       " ',',\n",
       " 'False',\n",
       " 'Negative',\n",
       " 'Rate',\n",
       " '(',\n",
       " 'FNR',\n",
       " ')',\n",
       " '.',\n",
       " 'Even',\n",
       " 'data',\n",
       " 'imbalanced',\n",
       " ',',\n",
       " 'figure',\n",
       " 'model',\n",
       " 'working',\n",
       " 'well',\n",
       " '.',\n",
       " 'For',\n",
       " ',',\n",
       " 'values',\n",
       " 'TPR',\n",
       " 'TNR',\n",
       " 'high',\n",
       " ',',\n",
       " 'FPR',\n",
       " 'FNR',\n",
       " 'low',\n",
       " 'possible',\n",
       " '.',\n",
       " 'With',\n",
       " 'help',\n",
       " 'TP',\n",
       " ',',\n",
       " 'TN',\n",
       " ',',\n",
       " 'FN',\n",
       " ',',\n",
       " 'FP',\n",
       " ',',\n",
       " 'performance',\n",
       " 'metrics',\n",
       " 'calculated',\n",
       " '.',\n",
       " 'Precision',\n",
       " ',',\n",
       " 'Recall',\n",
       " 'Both',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'crucial',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " ',',\n",
       " 'positive',\n",
       " 'class',\n",
       " 'mattered',\n",
       " 'compared',\n",
       " 'negative',\n",
       " '.',\n",
       " 'Why',\n",
       " '?',\n",
       " 'While',\n",
       " 'searching',\n",
       " 'something',\n",
       " 'web',\n",
       " ',',\n",
       " 'model',\n",
       " 'something',\n",
       " 'irrelevant',\n",
       " 'retrieved',\n",
       " '(',\n",
       " 'true',\n",
       " 'negative',\n",
       " 'case',\n",
       " ')',\n",
       " '.',\n",
       " 'Therefore',\n",
       " 'TP',\n",
       " ',',\n",
       " 'FP',\n",
       " ',',\n",
       " 'FN',\n",
       " 'used',\n",
       " 'Precision',\n",
       " 'Recall',\n",
       " '.',\n",
       " 'Precision',\n",
       " 'Out',\n",
       " 'positive',\n",
       " 'predicted',\n",
       " ',',\n",
       " 'percentage',\n",
       " 'truly',\n",
       " 'positive',\n",
       " '.',\n",
       " 'The',\n",
       " 'precision',\n",
       " 'value',\n",
       " 'lies',\n",
       " '0',\n",
       " '1',\n",
       " '.',\n",
       " 'Recall',\n",
       " 'Out',\n",
       " 'total',\n",
       " 'positive',\n",
       " ',',\n",
       " 'percentage',\n",
       " 'predicted',\n",
       " 'positive',\n",
       " '.',\n",
       " 'It',\n",
       " 'TPR',\n",
       " '(',\n",
       " 'true',\n",
       " 'positive',\n",
       " 'rate',\n",
       " ')',\n",
       " '.',\n",
       " 'How',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'useful',\n",
       " '?',\n",
       " 'Let',\n",
       " '’',\n",
       " 'see',\n",
       " 'examples',\n",
       " '.',\n",
       " 'EXAMPLE',\n",
       " '1-',\n",
       " 'Credit',\n",
       " 'card',\n",
       " 'fraud',\n",
       " 'detection',\n",
       " 'Confusion',\n",
       " 'Matrix',\n",
       " 'Credit',\n",
       " 'Card',\n",
       " 'Fraud',\n",
       " 'Detection',\n",
       " 'We',\n",
       " 'miss',\n",
       " 'fraud',\n",
       " 'transactions',\n",
       " '.',\n",
       " 'Therefore',\n",
       " ',',\n",
       " 'False-Negative',\n",
       " 'low',\n",
       " 'possible',\n",
       " '.',\n",
       " 'In',\n",
       " 'situations',\n",
       " ',',\n",
       " 'compromise',\n",
       " 'low',\n",
       " 'precision',\n",
       " ',',\n",
       " 'recall',\n",
       " 'high',\n",
       " '.',\n",
       " 'Similarly',\n",
       " ',',\n",
       " 'medical',\n",
       " 'application',\n",
       " ',',\n",
       " '’',\n",
       " 'miss',\n",
       " 'patient',\n",
       " '.',\n",
       " 'Therefore',\n",
       " 'focus',\n",
       " 'high',\n",
       " 'recall',\n",
       " '.',\n",
       " 'So',\n",
       " 'far',\n",
       " ',',\n",
       " 'discussed',\n",
       " 'recall',\n",
       " 'important',\n",
       " 'precision',\n",
       " '.',\n",
       " 'But',\n",
       " ',',\n",
       " 'precision',\n",
       " 'important',\n",
       " 'recall',\n",
       " '?',\n",
       " 'EXAMPLE',\n",
       " '2',\n",
       " '—',\n",
       " 'Spam',\n",
       " 'detection',\n",
       " 'Confusion',\n",
       " 'Matrix',\n",
       " 'Spam',\n",
       " 'detection',\n",
       " 'In',\n",
       " 'detection',\n",
       " 'spam',\n",
       " 'mail',\n",
       " ',',\n",
       " 'okay',\n",
       " 'spam',\n",
       " 'mail',\n",
       " 'remains',\n",
       " 'undetected',\n",
       " '(',\n",
       " 'false',\n",
       " 'negative',\n",
       " ')',\n",
       " ',',\n",
       " 'miss',\n",
       " 'critical',\n",
       " 'mail',\n",
       " 'classified',\n",
       " 'spam',\n",
       " '(',\n",
       " 'false',\n",
       " 'positive',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'situation',\n",
       " ',',\n",
       " 'False',\n",
       " 'Positive',\n",
       " 'low',\n",
       " 'possible',\n",
       " '.',\n",
       " 'Here',\n",
       " ',',\n",
       " 'precision',\n",
       " 'vital',\n",
       " 'compared',\n",
       " 'recall',\n",
       " '.',\n",
       " 'When',\n",
       " 'comparing',\n",
       " 'different',\n",
       " 'models',\n",
       " ',',\n",
       " 'difficult',\n",
       " 'decide',\n",
       " 'better',\n",
       " '(',\n",
       " 'high',\n",
       " 'precision',\n",
       " 'low',\n",
       " 'recall',\n",
       " 'vice-versa',\n",
       " ')',\n",
       " '.',\n",
       " 'Therefore',\n",
       " ',',\n",
       " 'metric',\n",
       " 'combines',\n",
       " '.',\n",
       " 'One',\n",
       " 'metric',\n",
       " 'F1',\n",
       " 'score',\n",
       " '.',\n",
       " 'F1',\n",
       " 'Score',\n",
       " 'It',\n",
       " 'harmonic',\n",
       " 'mean',\n",
       " 'precision',\n",
       " 'recall',\n",
       " '.',\n",
       " 'It',\n",
       " 'takes',\n",
       " 'false',\n",
       " 'positive',\n",
       " 'false',\n",
       " 'negatives',\n",
       " 'account',\n",
       " '.',\n",
       " 'Therefore',\n",
       " ',',\n",
       " 'performs',\n",
       " 'well',\n",
       " 'imbalanced',\n",
       " 'dataset',\n",
       " '.',\n",
       " 'F1',\n",
       " 'score',\n",
       " 'gives',\n",
       " 'weightage',\n",
       " 'recall',\n",
       " 'precision',\n",
       " '.',\n",
       " 'There',\n",
       " 'weighted',\n",
       " 'F1',\n",
       " 'score',\n",
       " 'give',\n",
       " 'different',\n",
       " 'weightage',\n",
       " 'recall',\n",
       " 'precision',\n",
       " '.',\n",
       " 'As',\n",
       " 'discussed',\n",
       " 'previous',\n",
       " 'section',\n",
       " ',',\n",
       " 'different',\n",
       " 'problems',\n",
       " 'give',\n",
       " 'different',\n",
       " 'weightage',\n",
       " 'recall',\n",
       " 'precision',\n",
       " '.',\n",
       " 'Beta',\n",
       " 'represents',\n",
       " 'many',\n",
       " 'times',\n",
       " 'recall',\n",
       " 'important',\n",
       " 'precision',\n",
       " '.',\n",
       " 'If',\n",
       " 'recall',\n",
       " 'twice',\n",
       " 'important',\n",
       " 'precision',\n",
       " ',',\n",
       " 'value',\n",
       " 'Beta',\n",
       " '2',\n",
       " '.',\n",
       " 'Conclusion',\n",
       " 'Confusion',\n",
       " 'matrix',\n",
       " ',',\n",
       " 'precision',\n",
       " ',',\n",
       " 'recall',\n",
       " ',',\n",
       " 'F1',\n",
       " 'score',\n",
       " 'provides',\n",
       " 'better',\n",
       " 'insights',\n",
       " 'prediction',\n",
       " 'compared',\n",
       " 'accuracy',\n",
       " 'performance',\n",
       " 'metrics',\n",
       " '.',\n",
       " 'Applications',\n",
       " 'precision',\n",
       " ',',\n",
       " 'recall',\n",
       " ',',\n",
       " 'F1',\n",
       " 'score',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " ',',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'many',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = []\n",
    "for sent in sentences:\n",
    "    type(sent)\n",
    "    text_tokens = word_tokenize(sent)\n",
    "    for word in text_tokens:\n",
    "        if not word in stopwords.words():\n",
    "            tokens.append(word)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8e9028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accuracy', 'performance', 'metrics', 'decisive', 'dealing', 'imbalanced', 'data', 'In', 'blog', 'learn', 'Confusion', 'matrix', 'associated', 'terms', 'looks', 'confusing', 'trivial', 'The', 'confusion', 'matrix', 'precision', 'recall', 'F1', 'score', 'gives', 'better', 'intuition', 'prediction', 'results', 'compared', 'accuracy', 'To', 'understand', 'concepts', 'limit', 'article', 'binary', 'classification', 'What', 'confusion', 'matrix', 'It', 'matrix', 'size', '2×2', 'binary', 'classification', 'actual', 'values', 'axis', 'predicted', 'another', 'Confusion', 'Matrix', 'Let', '’', 'understand', 'confusing', 'terms', 'confusion', 'matrix', 'true', 'positive', 'true', 'negative', 'false', 'negative', 'false', 'positive', 'example', 'EXAMPLE', 'A', 'machine', 'learning', 'model', 'trained', 'predict', 'tumor', 'patients', 'The', 'test', 'dataset', 'consists', '100', 'people', 'Confusion', 'Matrix', 'tumor', 'detection', 'True', 'Positive', 'TP', '—', 'model', 'correctly', 'predicts', 'positive', 'class', 'prediction', 'actual', 'positive', 'In', 'example', '10', 'people', 'tumors', 'predicted', 'positively', 'model', 'True', 'Negative', 'TN', '—', 'model', 'correctly', 'predicts', 'negative', 'class', 'prediction', 'actual', 'negative', 'In', 'example', '60', 'people', '’', 'tumors', 'predicted', 'negatively', 'model', 'False', 'Positive', 'FP', '—', 'model', 'gives', 'wrong', 'prediction', 'negative', 'class', 'predictedpositive', 'actualnegative', 'In', 'example', '22', 'people', 'predicted', 'positive', 'tumor', 'although', '’', 'tumor', 'FP', 'called', 'TYPE', 'I', 'error', 'False', 'Negative', 'FN', '—', 'model', 'wrongly', 'predicts', 'positive', 'class', 'predictednegative', 'actualpositive', 'In', 'example', '8', 'people', 'tumors', 'predicted', 'negative', 'FN', 'called', 'TYPE', 'II', 'error', 'With', 'help', 'four', 'values', 'calculate', 'True', 'Positive', 'Rate', 'TPR', 'False', 'Negative', 'Rate', 'FPR', 'True', 'Negative', 'Rate', 'TNR', 'False', 'Negative', 'Rate', 'FNR', 'Even', 'data', 'imbalanced', 'figure', 'model', 'working', 'well', 'For', 'values', 'TPR', 'TNR', 'high', 'FPR', 'FNR', 'low', 'possible', 'With', 'help', 'TP', 'TN', 'FN', 'FP', 'performance', 'metrics', 'calculated', 'Precision', 'Recall', 'Both', 'precision', 'recall', 'crucial', 'information', 'retrieval', 'positive', 'class', 'mattered', 'compared', 'negative', 'Why', 'While', 'searching', 'something', 'web', 'model', 'something', 'irrelevant', 'retrieved', 'true', 'negative', 'case', 'Therefore', 'TP', 'FP', 'FN', 'used', 'Precision', 'Recall', 'Precision', 'Out', 'positive', 'predicted', 'percentage', 'truly', 'positive', 'The', 'precision', 'value', 'lies', '0', '1', 'Recall', 'Out', 'total', 'positive', 'percentage', 'predicted', 'positive', 'It', 'TPR', 'true', 'positive', 'rate', 'How', 'precision', 'recall', 'useful', 'Let', '’', 'see', 'examples', 'EXAMPLE', '1', 'Credit', 'card', 'fraud', 'detection', 'Confusion', 'Matrix', 'Credit', 'Card', 'Fraud', 'Detection', 'We', 'miss', 'fraud', 'transactions', 'Therefore', 'FalseNegative', 'low', 'possible', 'In', 'situations', 'compromise', 'low', 'precision', 'recall', 'high', 'Similarly', 'medical', 'application', '’', 'miss', 'patient', 'Therefore', 'focus', 'high', 'recall', 'So', 'far', 'discussed', 'recall', 'important', 'precision', 'But', 'precision', 'important', 'recall', 'EXAMPLE', '2', '—', 'Spam', 'detection', 'Confusion', 'Matrix', 'Spam', 'detection', 'In', 'detection', 'spam', 'mail', 'okay', 'spam', 'mail', 'remains', 'undetected', 'false', 'negative', 'miss', 'critical', 'mail', 'classified', 'spam', 'false', 'positive', 'In', 'situation', 'False', 'Positive', 'low', 'possible', 'Here', 'precision', 'vital', 'compared', 'recall', 'When', 'comparing', 'different', 'models', 'difficult', 'decide', 'better', 'high', 'precision', 'low', 'recall', 'viceversa', 'Therefore', 'metric', 'combines', 'One', 'metric', 'F1', 'score', 'F1', 'Score', 'It', 'harmonic', 'mean', 'precision', 'recall', 'It', 'takes', 'false', 'positive', 'false', 'negatives', 'account', 'Therefore', 'performs', 'well', 'imbalanced', 'dataset', 'F1', 'score', 'gives', 'weightage', 'recall', 'precision', 'There', 'weighted', 'F1', 'score', 'give', 'different', 'weightage', 'recall', 'precision', 'As', 'discussed', 'previous', 'section', 'different', 'problems', 'give', 'different', 'weightage', 'recall', 'precision', 'Beta', 'represents', 'many', 'times', 'recall', 'important', 'precision', 'If', 'recall', 'twice', 'important', 'precision', 'value', 'Beta', '2', 'Conclusion', 'Confusion', 'matrix', 'precision', 'recall', 'F1', 'score', 'provides', 'better', 'insights', 'prediction', 'compared', 'accuracy', 'performance', 'metrics', 'Applications', 'precision', 'recall', 'F1', 'score', 'information', 'retrieval', 'word', 'segmentation', 'named', 'entity', 'recognition', 'many']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "tokens = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "\n",
    "tokens = [s for s in tokens if s]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5941eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuraci', 'perform', 'metric', 'decis', 'deal', 'imbalanc', 'data', 'In', 'blog', 'learn', 'confus', 'matrix', 'associ', 'term', 'look', 'confus', 'trivial', 'the', 'confus', 'matrix', 'precis', 'recal', 'F1', 'score', 'give', 'better', 'intuit', 'predict', 'result', 'compar', 'accuraci', 'To', 'understand', 'concept', 'limit', 'articl', 'binari', 'classif', 'what', 'confus', 'matrix', 'It', 'matrix', 'size', '2×2', 'binari', 'classif', 'actual', 'valu', 'axi', 'predict', 'anoth', 'confus', 'matrix', 'let', '’', 'understand', 'confus', 'term', 'confus', 'matrix', 'true', 'posit', 'true', 'neg', 'fals', 'neg', 'fals', 'posit', 'exampl', 'exampl', 'A', 'machin', 'learn', 'model', 'train', 'predict', 'tumor', 'patient', 'the', 'test', 'dataset', 'consist', '100', 'peopl', 'confus', 'matrix', 'tumor', 'detect', 'true', 'posit', 'TP', '—', 'model', 'correctli', 'predict', 'posit', 'class', 'predict', 'actual', 'posit', 'In', 'exampl', '10', 'peopl', 'tumor', 'predict', 'posit', 'model', 'true', 'neg', 'TN', '—', 'model', 'correctli', 'predict', 'neg', 'class', 'predict', 'actual', 'neg', 'In', 'exampl', '60', 'peopl', '’', 'tumor', 'predict', 'neg', 'model', 'fals', 'posit', 'FP', '—', 'model', 'give', 'wrong', 'predict', 'neg', 'class', 'predictedposit', 'actualneg', 'In', 'exampl', '22', 'peopl', 'predict', 'posit', 'tumor', 'although', '’', 'tumor', 'FP', 'call', 'type', 'I', 'error', 'fals', 'neg', 'FN', '—', 'model', 'wrongli', 'predict', 'posit', 'class', 'predictedneg', 'actualposit', 'In', 'exampl', '8', 'peopl', 'tumor', 'predict', 'neg', 'FN', 'call', 'type', 'II', 'error', 'with', 'help', 'four', 'valu', 'calcul', 'true', 'posit', 'rate', 'tpr', 'fals', 'neg', 'rate', 'fpr', 'true', 'neg', 'rate', 'tnr', 'fals', 'neg', 'rate', 'fnr', 'even', 'data', 'imbalanc', 'figur', 'model', 'work', 'well', 'for', 'valu', 'tpr', 'tnr', 'high', 'fpr', 'fnr', 'low', 'possibl', 'with', 'help', 'TP', 'TN', 'FN', 'FP', 'perform', 'metric', 'calcul', 'precis', 'recal', 'both', 'precis', 'recal', 'crucial', 'inform', 'retriev', 'posit', 'class', 'matter', 'compar', 'neg', 'whi', 'while', 'search', 'someth', 'web', 'model', 'someth', 'irrelev', 'retriev', 'true', 'neg', 'case', 'therefor', 'TP', 'FP', 'FN', 'use', 'precis', 'recal', 'precis', 'out', 'posit', 'predict', 'percentag', 'truli', 'posit', 'the', 'precis', 'valu', 'lie', '0', '1', 'recal', 'out', 'total', 'posit', 'percentag', 'predict', 'posit', 'It', 'tpr', 'true', 'posit', 'rate', 'how', 'precis', 'recal', 'use', 'let', '’', 'see', 'exampl', 'exampl', '1', 'credit', 'card', 'fraud', 'detect', 'confus', 'matrix', 'credit', 'card', 'fraud', 'detect', 'We', 'miss', 'fraud', 'transact', 'therefor', 'falseneg', 'low', 'possibl', 'In', 'situat', 'compromis', 'low', 'precis', 'recal', 'high', 'similarli', 'medic', 'applic', '’', 'miss', 'patient', 'therefor', 'focu', 'high', 'recal', 'So', 'far', 'discuss', 'recal', 'import', 'precis', 'but', 'precis', 'import', 'recal', 'exampl', '2', '—', 'spam', 'detect', 'confus', 'matrix', 'spam', 'detect', 'In', 'detect', 'spam', 'mail', 'okay', 'spam', 'mail', 'remain', 'undetect', 'fals', 'neg', 'miss', 'critic', 'mail', 'classifi', 'spam', 'fals', 'posit', 'In', 'situat', 'fals', 'posit', 'low', 'possibl', 'here', 'precis', 'vital', 'compar', 'recal', 'when', 'compar', 'differ', 'model', 'difficult', 'decid', 'better', 'high', 'precis', 'low', 'recal', 'viceversa', 'therefor', 'metric', 'combin', 'one', 'metric', 'F1', 'score', 'F1', 'score', 'It', 'harmon', 'mean', 'precis', 'recal', 'It', 'take', 'fals', 'posit', 'fals', 'neg', 'account', 'therefor', 'perform', 'well', 'imbalanc', 'dataset', 'F1', 'score', 'give', 'weightag', 'recal', 'precis', 'there', 'weight', 'F1', 'score', 'give', 'differ', 'weightag', 'recal', 'precis', 'As', 'discuss', 'previou', 'section', 'differ', 'problem', 'give', 'differ', 'weightag', 'recal', 'precis', 'beta', 'repres', 'mani', 'time', 'recal', 'import', 'precis', 'If', 'recal', 'twice', 'import', 'precis', 'valu', 'beta', '2', 'conclus', 'confus', 'matrix', 'precis', 'recal', 'F1', 'score', 'provid', 'better', 'insight', 'predict', 'compar', 'accuraci', 'perform', 'metric', 'applic', 'precis', 'recal', 'F1', 'score', 'inform', 'retriev', 'word', 'segment', 'name', 'entiti', 'recognit', 'mani']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "   \n",
    "ps = PorterStemmer()\n",
    "  \n",
    "stem_tokens = []\n",
    "for w in tokens:\n",
    "    stem_tokens.append(ps.stem(w))\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c003963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuraci perform metric decis deal imbalanc data In blog learn confus matrix associ term look confus trivial the confus matrix precis recal F1 score give better intuit predict result compar accuraci To understand concept limit articl binari classif what confus matrix It matrix size 2×2 binari classif actual valu axi predict anoth confus matrix let ’ understand confus term confus matrix true posit true neg fals neg fals posit exampl exampl A machin learn model train predict tumor patient the test dataset consist 100 peopl confus matrix tumor detect true posit TP — model correctli predict posit class predict actual posit In exampl 10 peopl tumor predict posit model true neg TN — model correctli predict neg class predict actual neg In exampl 60 peopl ’ tumor predict neg model fals posit FP — model give wrong predict neg class predictedposit actualneg In exampl 22 peopl predict posit tumor although ’ tumor FP call type I error fals neg FN — model wrongli predict posit class predictedneg actualposit In exampl 8 peopl tumor predict neg FN call type II error with help four valu calcul true posit rate tpr fals neg rate fpr true neg rate tnr fals neg rate fnr even data imbalanc figur model work well for valu tpr tnr high fpr fnr low possibl with help TP TN FN FP perform metric calcul precis recal both precis recal crucial inform retriev posit class matter compar neg whi while search someth web model someth irrelev retriev true neg case therefor TP FP FN use precis recal precis out posit predict percentag truli posit the precis valu lie 0 1 recal out total posit percentag predict posit It tpr true posit rate how precis recal use let ’ see exampl exampl 1 credit card fraud detect confus matrix credit card fraud detect We miss fraud transact therefor falseneg low possibl In situat compromis low precis recal high similarli medic applic ’ miss patient therefor focu high recal So far discuss recal import precis but precis import recal exampl 2 — spam detect confus matrix spam detect In detect spam mail okay spam mail remain undetect fals neg miss critic mail classifi spam fals posit In situat fals posit low possibl here precis vital compar recal when compar differ model difficult decid better high precis low recal viceversa therefor metric combin one metric F1 score F1 score It harmon mean precis recal It take fals posit fals neg account therefor perform well imbalanc dataset F1 score give weightag recal precis there weight F1 score give differ weightag recal precis As discuss previou section differ problem give differ weightag recal precis beta repres mani time recal import precis If recal twice import precis valu beta 2 conclus confus matrix precis recal F1 score provid better insight predict compar accuraci perform metric applic precis recal F1 score inform retriev word segment name entiti recognit mani'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = (\" \".join(stem_tokens))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72448e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2842"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"results.txt\", \"a\")\n",
    "f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0c33a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "boto3.Session().resource(service_name='s3',\n",
    "aws_access_key_id='AKIAUSEODYLG6RNKOVEM',\n",
    "aws_secret_access_key='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').Bucket(\"pipozbuck\").Object('results.txt').upload_file('results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b2efb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuraci perform metric decis deal imbalanc data In blog learn confus matrix associ term look confus trivial the confus matrix precis recal F1 score give better intuit predict result compar accuraci To understand concept limit articl binari classif what confus matrix It matrix size 2×2 binari classif actual valu axi predict anoth confus matrix let ’ understand confus term confus matrix true posit true neg fals neg fals posit exampl exampl A machin learn model train predict tumor patient the test dataset consist 100 peopl confus matrix tumor detect true posit TP — model correctli predict posit class predict actual posit In exampl 10 peopl tumor predict posit model true neg TN — model correctli predict neg class predict actual neg In exampl 60 peopl ’ tumor predict neg model fals posit FP — model give wrong predict neg class predictedposit actualneg In exampl 22 peopl predict posit tumor although ’ tumor FP call type I error fals neg FN — model wrongli predict posit class predictedneg actualposit In exampl 8 peopl tumor predict neg FN call type II error with help four valu calcul true posit rate tpr fals neg rate fpr true neg rate tnr fals neg rate fnr even data imbalanc figur model work well for valu tpr tnr high fpr fnr low possibl with help TP TN FN FP perform metric calcul precis recal both precis recal crucial inform retriev posit class matter compar neg whi while search someth web model someth irrelev retriev true neg case therefor TP FP FN use precis recal precis out posit predict percentag truli posit the precis valu lie 0 1 recal out total posit percentag predict posit It tpr true posit rate how precis recal use let ’ see exampl exampl 1 credit card fraud detect confus matrix credit card fraud detect We miss fraud transact therefor falseneg low possibl In situat compromis low precis recal high similarli medic applic ’ miss patient therefor focu high recal So far discuss recal import precis but precis import recal exampl 2 — spam detect confus matrix spam detect In detect spam mail okay spam mail remain undetect fals neg miss critic mail classifi spam fals posit In situat fals posit low possibl here precis vital compar recal when compar differ model difficult decid better high precis low recal viceversa therefor metric combin one metric F1 score F1 score It harmon mean precis recal It take fals posit fals neg account therefor perform well imbalanc dataset F1 score give weightag recal precis there weight F1 score give differ weightag recal precis As discuss previou section differ problem give differ weightag recal precis beta repres mani time recal import precis If recal twice import precis valu beta 2 conclus confus matrix precis recal F1 score provid better insight predict compar accuraci perform metric applic precis recal F1 score inform retriev word segment name entiti recognit mani'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = s3.get_object(Bucket = 'pipozbuck',Key = 'results.txt')\n",
    "with io.FileIO('sample.txt', 'w') as file:\n",
    "    for i in obj['Body']:\n",
    "        file.write(i)\n",
    "f = open(\"sample.txt\", \"r\")\n",
    "result_data = f.read()\n",
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4104eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuraci perform metric decis deal imbalanc data In blog learn confus matrix associ term look confus trivial the confus matrix precis recal F1 score give better intuit predict result compar accuraci To understand concept limit articl binari classif what confus matrix It matrix size 2×2 binari classif actual valu axi predict anoth confus matrix let ’ understand confus term confus matrix true posit true neg fals neg fals posit exampl exampl A machin learn model train predict tumor patient the test dataset consist 100 peopl confus matrix tumor detect true posit TP — model correctli predict posit class predict actual posit In exampl 10 peopl tumor predict posit model true neg TN — model correctli predict neg class predict actual neg In exampl 60 peopl ’ tumor predict neg model fals posit FP — model give wrong predict neg class predictedposit actualneg In exampl 22 peopl predict posit tumor although ’ tumor FP call type I error fals neg FN — model wrongli predict posit class predictedneg actualposit In exampl 8 peopl tumor predict neg FN call type II error with help four valu calcul true posit rate tpr fals neg rate fpr true neg rate tnr fals neg rate fnr even data imbalanc figur model work well for valu tpr tnr high fpr fnr low possibl with help TP TN FN FP perform metric calcul precis recal both precis recal crucial inform retriev posit class matter compar neg whi while search someth web model someth irrelev retriev true neg case therefor TP FP FN use precis recal precis out posit predict percentag truli posit the precis valu lie 0 1 recal out total posit percentag predict posit It tpr true posit rate how precis recal use let ’ see exampl exampl 1 credit card fraud detect confus matrix credit card fraud detect We miss fraud transact therefor falseneg low possibl In situat compromis low precis recal high similarli medic applic ’ miss patient therefor focu high recal So far discuss recal import precis but precis import recal exampl 2 — spam detect confus matrix spam detect In detect spam mail okay spam mail remain undetect fals neg miss critic mail classifi spam fals posit In situat fals posit low possibl here precis vital compar recal when compar differ model difficult decid better high precis low recal viceversa therefor metric combin one metric F1 score F1 score It harmon mean precis recal It take fals posit fals neg account therefor perform well imbalanc dataset F1 score give weightag recal precis there weight F1 score give differ weightag recal precis As discuss previou section differ problem give differ weightag recal precis beta repres mani time recal import precis If recal twice import precis valu beta 2 conclus confus matrix precis recal F1 score provid better insight predict compar accuraci perform metric applic precis recal F1 score inform retriev word segment name entiti recognit mani'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63efc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoded_word = encoder.fit_transform(result_data.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74f4bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encoded_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6503bb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25, 118, 107,  63,  61,  88,  59,  17,  38,  94,  53, 103,  33,\n",
       "       146,  98,  53, 157, 148,  53, 103, 121, 129,  11, 135,  82,  36,\n",
       "        92, 122, 133,  49,  25,  22, 163,  51,  97,  32,  37,  46, 173,\n",
       "        53, 103,  18, 103, 142,   6,  37,  46,  26, 166,  34, 122,  30,\n",
       "        53, 103,  95, 183, 163,  53, 146,  53, 103, 158, 119, 158, 111,\n",
       "        72, 111,  72, 119,  71,  71,   9, 100,  94, 109, 155, 122, 160,\n",
       "       115, 148, 147,  60,  54,   3, 116,  53, 103, 160,  64, 158, 119,\n",
       "        21, 182, 109,  55, 122, 119,  45, 122,  26, 119,  17,  71,   2,\n",
       "       116, 160, 122, 119, 109, 158, 111,  20, 182, 109,  55, 122, 111,\n",
       "        45, 122,  26, 111,  17,  71,   7, 116, 183, 160, 122, 111, 109,\n",
       "        72, 119,  13, 182, 109,  82, 180, 122, 111,  45, 124,  27,  17,\n",
       "        71,   5, 116, 122, 119, 160,  29, 183, 160,  13,  42, 162,  14,\n",
       "        69,  72, 111,  12, 182, 109, 181, 122, 119,  45, 123,  28,  17,\n",
       "        71,   8, 116, 160, 122, 111,  12,  42, 162,  15,  69, 177,  84,\n",
       "        79, 166,  41, 158, 119, 128, 154,  72, 111, 128,  80, 158, 111,\n",
       "       128, 152,  72, 111, 128,  76,  70,  59,  88,  75, 109, 179, 172,\n",
       "        78, 166, 154, 152,  86,  80,  76,  99, 120, 177,  84,  21,  20,\n",
       "        12,  13, 118, 107,  41, 121, 129,  39, 121, 129,  58,  90, 134,\n",
       "       119,  45, 104,  49, 111, 175, 176, 136, 143, 169, 109, 143,  93,\n",
       "       134, 158, 111,  44, 150,  21,  13,  12, 165, 121, 129, 121, 114,\n",
       "       119, 122, 117, 159, 119, 148, 121, 166,  96,   0,   1, 129, 114,\n",
       "       153, 119, 117, 122, 119,  18, 154, 158, 119, 128,  87, 121, 129,\n",
       "       165,  95, 183, 138,  71,  71,   1,  56,  43,  81,  64,  53, 103,\n",
       "        56,  43,  81,  64,  23, 108,  81, 156, 150,  73,  99, 120,  17,\n",
       "       141,  50,  99, 121, 129,  86, 140, 106,  31, 183, 108, 115, 150,\n",
       "        77,  86, 129,  19,  74,  67, 129,  89, 121,  40, 121,  89, 129,\n",
       "        71,   4, 182, 144,  64,  53, 103, 144,  64,  17,  64, 144, 101,\n",
       "       112, 144, 101, 131, 164,  72, 111, 108,  57, 101,  47, 144,  72,\n",
       "       119,  17, 141,  72, 119,  99, 120,  85, 121, 168,  49, 129, 174,\n",
       "        49,  65, 109,  66,  62,  36,  86, 121,  99, 129, 167, 150, 107,\n",
       "        48, 113, 107,  11, 135,  11, 135,  18,  83, 105, 121, 129,  18,\n",
       "       145,  72, 119,  72, 111,  24, 150, 118, 172,  88,  60,  11, 135,\n",
       "        82, 171, 129, 121, 149, 170,  11, 135,  82,  65, 171, 129, 121,\n",
       "        10,  67, 125, 137,  65, 126,  82,  65, 171, 129, 121,  35, 132,\n",
       "       102, 151, 129,  89, 121,  16, 129, 161,  89, 121, 166,  35,   4,\n",
       "        52,  53, 103, 121, 129,  11, 135, 127,  36,  91, 122,  49,  25,\n",
       "       118, 107,  31, 121, 129,  11, 135,  90, 134, 178, 139, 110,  68,\n",
       "       130, 102])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "121e4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "K = 3\n",
    "class point():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.k = np.random.randint(0,K)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str({\"data\":self.data, \"k\":self.k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93fe814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [point(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edecb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def make_k_mapping(points):\n",
    "    point_dict = defaultdict(list)\n",
    "    for p in points:\n",
    "        point_dict[p.k] = point_dict[p.k] + [p.data]\n",
    "    return point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c4c2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_k_means(point_dict):\n",
    "    means = [np.mean(point_dict[k],axis=0) for k in range(K)]\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebb0bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_k(points,means):\n",
    "    for p in points:   \n",
    "        dists = [np.linalg.norm(means[k]-p.data) for k in range(K)]\n",
    "        p.k = np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf428258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(points, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        point_dict = make_k_mapping(points)\n",
    "        means = calc_k_means(point_dict)\n",
    "        update_k(points, means)\n",
    "    return means, points\n",
    "new_means, new_points = fit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9c2a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(points):\n",
    "    point_dict = make_k_mapping(points)\n",
    "    means = calc_k_means(point_dict)\n",
    "    dists = [np.linalg.norm(means[p.k]-p.data) for p in points]\n",
    "    return np.mean(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5d06d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103.4126213592233, 35.436619718309856, 151.71851851851852]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'data': 25, 'k': 1},\n",
       " {'data': 118, 'k': 0},\n",
       " {'data': 107, 'k': 0},\n",
       " {'data': 63, 'k': 1},\n",
       " {'data': 61, 'k': 1},\n",
       " {'data': 88, 'k': 0},\n",
       " {'data': 59, 'k': 1},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 38, 'k': 1},\n",
       " {'data': 94, 'k': 0},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 33, 'k': 1},\n",
       " {'data': 146, 'k': 2},\n",
       " {'data': 98, 'k': 0},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 157, 'k': 2},\n",
       " {'data': 148, 'k': 2},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 82, 'k': 0},\n",
       " {'data': 36, 'k': 1},\n",
       " {'data': 92, 'k': 0},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 133, 'k': 2},\n",
       " {'data': 49, 'k': 1},\n",
       " {'data': 25, 'k': 1},\n",
       " {'data': 22, 'k': 1},\n",
       " {'data': 163, 'k': 2},\n",
       " {'data': 51, 'k': 1},\n",
       " {'data': 97, 'k': 0},\n",
       " {'data': 32, 'k': 1},\n",
       " {'data': 37, 'k': 1},\n",
       " {'data': 46, 'k': 1},\n",
       " {'data': 173, 'k': 2},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 18, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 142, 'k': 2},\n",
       " {'data': 6, 'k': 1},\n",
       " {'data': 37, 'k': 1},\n",
       " {'data': 46, 'k': 1},\n",
       " {'data': 26, 'k': 1},\n",
       " {'data': 166, 'k': 2},\n",
       " {'data': 34, 'k': 1},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 30, 'k': 1},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 95, 'k': 0},\n",
       " {'data': 183, 'k': 2},\n",
       " {'data': 163, 'k': 2},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 146, 'k': 2},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 9, 'k': 1},\n",
       " {'data': 100, 'k': 0},\n",
       " {'data': 94, 'k': 0},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 155, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 115, 'k': 0},\n",
       " {'data': 148, 'k': 2},\n",
       " {'data': 147, 'k': 2},\n",
       " {'data': 60, 'k': 1},\n",
       " {'data': 54, 'k': 1},\n",
       " {'data': 3, 'k': 1},\n",
       " {'data': 116, 'k': 0},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 21, 'k': 1},\n",
       " {'data': 182, 'k': 2},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 55, 'k': 1},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 45, 'k': 1},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 26, 'k': 1},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 2, 'k': 1},\n",
       " {'data': 116, 'k': 0},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 20, 'k': 1},\n",
       " {'data': 182, 'k': 2},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 55, 'k': 1},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 45, 'k': 1},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 26, 'k': 1},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 7, 'k': 1},\n",
       " {'data': 116, 'k': 0},\n",
       " {'data': 183, 'k': 2},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 13, 'k': 1},\n",
       " {'data': 182, 'k': 2},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 82, 'k': 0},\n",
       " {'data': 180, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 45, 'k': 1},\n",
       " {'data': 124, 'k': 0},\n",
       " {'data': 27, 'k': 1},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 5, 'k': 1},\n",
       " {'data': 116, 'k': 0},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 29, 'k': 1},\n",
       " {'data': 183, 'k': 2},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 13, 'k': 1},\n",
       " {'data': 42, 'k': 1},\n",
       " {'data': 162, 'k': 2},\n",
       " {'data': 14, 'k': 1},\n",
       " {'data': 69, 'k': 1},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 12, 'k': 1},\n",
       " {'data': 182, 'k': 2},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 181, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 45, 'k': 1},\n",
       " {'data': 123, 'k': 0},\n",
       " {'data': 28, 'k': 1},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 8, 'k': 1},\n",
       " {'data': 116, 'k': 0},\n",
       " {'data': 160, 'k': 2},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 12, 'k': 1},\n",
       " {'data': 42, 'k': 1},\n",
       " {'data': 162, 'k': 2},\n",
       " {'data': 15, 'k': 1},\n",
       " {'data': 69, 'k': 1},\n",
       " {'data': 177, 'k': 2},\n",
       " {'data': 84, 'k': 0},\n",
       " {'data': 79, 'k': 0},\n",
       " {'data': 166, 'k': 2},\n",
       " {'data': 41, 'k': 1},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 128, 'k': 2},\n",
       " {'data': 154, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 128, 'k': 2},\n",
       " {'data': 80, 'k': 0},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 128, 'k': 2},\n",
       " {'data': 152, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 128, 'k': 2},\n",
       " {'data': 76, 'k': 0},\n",
       " {'data': 70, 'k': 0},\n",
       " {'data': 59, 'k': 1},\n",
       " {'data': 88, 'k': 0},\n",
       " {'data': 75, 'k': 0},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 179, 'k': 2},\n",
       " {'data': 172, 'k': 2},\n",
       " {'data': 78, 'k': 0},\n",
       " {'data': 166, 'k': 2},\n",
       " {'data': 154, 'k': 2},\n",
       " {'data': 152, 'k': 2},\n",
       " {'data': 86, 'k': 0},\n",
       " {'data': 80, 'k': 0},\n",
       " {'data': 76, 'k': 0},\n",
       " {'data': 99, 'k': 0},\n",
       " {'data': 120, 'k': 0},\n",
       " {'data': 177, 'k': 2},\n",
       " {'data': 84, 'k': 0},\n",
       " {'data': 21, 'k': 1},\n",
       " {'data': 20, 'k': 1},\n",
       " {'data': 12, 'k': 1},\n",
       " {'data': 13, 'k': 1},\n",
       " {'data': 118, 'k': 0},\n",
       " {'data': 107, 'k': 0},\n",
       " {'data': 41, 'k': 1},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 39, 'k': 1},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 58, 'k': 1},\n",
       " {'data': 90, 'k': 0},\n",
       " {'data': 134, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 45, 'k': 1},\n",
       " {'data': 104, 'k': 0},\n",
       " {'data': 49, 'k': 1},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 175, 'k': 2},\n",
       " {'data': 176, 'k': 2},\n",
       " {'data': 136, 'k': 2},\n",
       " {'data': 143, 'k': 2},\n",
       " {'data': 169, 'k': 2},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 143, 'k': 2},\n",
       " {'data': 93, 'k': 0},\n",
       " {'data': 134, 'k': 2},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 44, 'k': 1},\n",
       " {'data': 150, 'k': 2},\n",
       " {'data': 21, 'k': 1},\n",
       " {'data': 13, 'k': 1},\n",
       " {'data': 12, 'k': 1},\n",
       " {'data': 165, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 114, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 117, 'k': 0},\n",
       " {'data': 159, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 148, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 166, 'k': 2},\n",
       " {'data': 96, 'k': 0},\n",
       " {'data': 0, 'k': 1},\n",
       " {'data': 1, 'k': 1},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 114, 'k': 0},\n",
       " {'data': 153, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 117, 'k': 0},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 18, 'k': 1},\n",
       " {'data': 154, 'k': 2},\n",
       " {'data': 158, 'k': 2},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 128, 'k': 2},\n",
       " {'data': 87, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 165, 'k': 2},\n",
       " {'data': 95, 'k': 0},\n",
       " {'data': 183, 'k': 2},\n",
       " {'data': 138, 'k': 2},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 1, 'k': 1},\n",
       " {'data': 56, 'k': 1},\n",
       " {'data': 43, 'k': 1},\n",
       " {'data': 81, 'k': 0},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 56, 'k': 1},\n",
       " {'data': 43, 'k': 1},\n",
       " {'data': 81, 'k': 0},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 23, 'k': 1},\n",
       " {'data': 108, 'k': 0},\n",
       " {'data': 81, 'k': 0},\n",
       " {'data': 156, 'k': 2},\n",
       " {'data': 150, 'k': 2},\n",
       " {'data': 73, 'k': 0},\n",
       " {'data': 99, 'k': 0},\n",
       " {'data': 120, 'k': 0},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 141, 'k': 2},\n",
       " {'data': 50, 'k': 1},\n",
       " {'data': 99, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 86, 'k': 0},\n",
       " {'data': 140, 'k': 2},\n",
       " {'data': 106, 'k': 0},\n",
       " {'data': 31, 'k': 1},\n",
       " {'data': 183, 'k': 2},\n",
       " {'data': 108, 'k': 0},\n",
       " {'data': 115, 'k': 0},\n",
       " {'data': 150, 'k': 2},\n",
       " {'data': 77, 'k': 0},\n",
       " {'data': 86, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 19, 'k': 1},\n",
       " {'data': 74, 'k': 0},\n",
       " {'data': 67, 'k': 1},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 89, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 40, 'k': 1},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 89, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 71, 'k': 0},\n",
       " {'data': 4, 'k': 1},\n",
       " {'data': 182, 'k': 2},\n",
       " {'data': 144, 'k': 2},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 144, 'k': 2},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 64, 'k': 1},\n",
       " {'data': 144, 'k': 2},\n",
       " {'data': 101, 'k': 0},\n",
       " {'data': 112, 'k': 0},\n",
       " {'data': 144, 'k': 2},\n",
       " {'data': 101, 'k': 0},\n",
       " {'data': 131, 'k': 2},\n",
       " {'data': 164, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 108, 'k': 0},\n",
       " {'data': 57, 'k': 1},\n",
       " {'data': 101, 'k': 0},\n",
       " {'data': 47, 'k': 1},\n",
       " {'data': 144, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 17, 'k': 1},\n",
       " {'data': 141, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 99, 'k': 0},\n",
       " {'data': 120, 'k': 0},\n",
       " {'data': 85, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 168, 'k': 2},\n",
       " {'data': 49, 'k': 1},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 174, 'k': 2},\n",
       " {'data': 49, 'k': 1},\n",
       " {'data': 65, 'k': 1},\n",
       " {'data': 109, 'k': 0},\n",
       " {'data': 66, 'k': 1},\n",
       " {'data': 62, 'k': 1},\n",
       " {'data': 36, 'k': 1},\n",
       " {'data': 86, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 99, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 167, 'k': 2},\n",
       " {'data': 150, 'k': 2},\n",
       " {'data': 107, 'k': 0},\n",
       " {'data': 48, 'k': 1},\n",
       " {'data': 113, 'k': 0},\n",
       " {'data': 107, 'k': 0},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 18, 'k': 1},\n",
       " {'data': 83, 'k': 0},\n",
       " {'data': 105, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 18, 'k': 1},\n",
       " {'data': 145, 'k': 2},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 119, 'k': 0},\n",
       " {'data': 72, 'k': 0},\n",
       " {'data': 111, 'k': 0},\n",
       " {'data': 24, 'k': 1},\n",
       " {'data': 150, 'k': 2},\n",
       " {'data': 118, 'k': 0},\n",
       " {'data': 172, 'k': 2},\n",
       " {'data': 88, 'k': 0},\n",
       " {'data': 60, 'k': 1},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 82, 'k': 0},\n",
       " {'data': 171, 'k': 2},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 149, 'k': 2},\n",
       " {'data': 170, 'k': 2},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 82, 'k': 0},\n",
       " {'data': 65, 'k': 1},\n",
       " {'data': 171, 'k': 2},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 10, 'k': 1},\n",
       " {'data': 67, 'k': 1},\n",
       " {'data': 125, 'k': 0},\n",
       " {'data': 137, 'k': 2},\n",
       " {'data': 65, 'k': 1},\n",
       " {'data': 126, 'k': 0},\n",
       " {'data': 82, 'k': 0},\n",
       " {'data': 65, 'k': 1},\n",
       " {'data': 171, 'k': 2},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 35, 'k': 1},\n",
       " {'data': 132, 'k': 2},\n",
       " {'data': 102, 'k': 0},\n",
       " {'data': 151, 'k': 2},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 89, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 16, 'k': 1},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 161, 'k': 2},\n",
       " {'data': 89, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 166, 'k': 2},\n",
       " {'data': 35, 'k': 1},\n",
       " {'data': 4, 'k': 1},\n",
       " {'data': 52, 'k': 1},\n",
       " {'data': 53, 'k': 1},\n",
       " {'data': 103, 'k': 0},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 127, 'k': 0},\n",
       " {'data': 36, 'k': 1},\n",
       " {'data': 91, 'k': 0},\n",
       " {'data': 122, 'k': 0},\n",
       " {'data': 49, 'k': 1},\n",
       " {'data': 25, 'k': 1},\n",
       " {'data': 118, 'k': 0},\n",
       " {'data': 107, 'k': 0},\n",
       " {'data': 31, 'k': 1},\n",
       " {'data': 121, 'k': 0},\n",
       " {'data': 129, 'k': 2},\n",
       " {'data': 11, 'k': 1},\n",
       " {'data': 135, 'k': 2},\n",
       " {'data': 90, 'k': 0},\n",
       " {'data': 134, 'k': 2},\n",
       " {'data': 178, 'k': 2},\n",
       " {'data': 139, 'k': 2},\n",
       " {'data': 110, 'k': 0},\n",
       " {'data': 68, 'k': 1},\n",
       " {'data': 130, 'k': 2},\n",
       " {'data': 102, 'k': 0}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_means)\n",
    "new_points"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10c0eea4947e627f41ae212ed997354d807a69e817346ad66573ec79cd8a274b"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
